import math
import tensorflow as tf
import numpy as np
import os
from os.path import expanduser

NN_MODEL_STATE_DIR = "ai-code-scanner-nn-state-test"

OP_TRAINING = 1
OP_TESTING = 2
OP_PREDICT = 3

LOGDIR = "/home/tharanga/tensorflow_logs"


class NeuralNetworkModel:

    def __init__(self, nn_config):
        self.nn_config = nn_config

    def train(self, batch_x, batch_y):
        self.model(batch_x, batch_y, OP_TRAINING)

    def test(self, batch_x, batch_y):
        return self.model(batch_x, batch_y, OP_TESTING)

    def predict(self, batch_x):
        return self.model(batch_x, None, OP_PREDICT)

    def model(self, batch_x, batch_y, op):

        input_size = self.nn_config.input_size
        input_channel_size = 3 # channel_1 : node_type_1, channel_2 : edge_type, channel_2 : node_type_2

        label_class_size = self.nn_config.label_class_size
        iterations = self.nn_config.iterations

        nn_model_state_dir = NeuralNetworkModel.get_nn_model_ckpt()

        # TODO last_layer_size calculation
        # eg: 28x28 > 14x14 > 7x7
        # https://www.quora.com/How-can-I-calculate-the-size-of-output-of-convolutional-layer
        last_layer_size = (input_size/2)/2/2

        # input
        x = tf.placeholder(tf.float32, [None, input_size, input_size, input_channel_size])

        with tf.name_scope("input"):
            x_visual = tf.reshape(x, [-1, input_size, input_size, input_channel_size])
            tf.summary.image('input', x_visual, 50)
            in_summ = tf.summary.merge_all()

        # labels
        _y = tf.placeholder(tf.float32, [None, label_class_size])

        # step for variable learning rate
        step = tf.placeholder(tf.int32)

        # three convolutional layers with their channel counts, and a
        k = 4  # first convolutional layer output depth
        l = 8  # second convolutional layer output depth
        m = 16  # third convolutional layer

        d = 200  # fully connected layer

        # convolutional layer #1
        stride = 2
        conv1 = tf.layers.conv2d(x, filters=k, kernel_size=[10, 10], padding="same", activation=tf.nn.relu)
        pool1 = tf.layers.max_pooling2d(conv1, pool_size=[2, 2], strides=stride)

        # convolutional layer #2
        stride = 2
        conv2 = tf.layers.conv2d(pool1, filters=l, kernel_size=[5, 5], padding="same", activation=tf.nn.relu)
        pool2 = tf.layers.max_pooling2d(conv2, pool_size=[2, 2], strides=stride)

        # convolutional layer #3
        stride = 2
        conv3 = tf.layers.conv2d(pool2, filters=m, kernel_size=[5, 5], padding="same", activation=tf.nn.relu)
        pool3 = tf.layers.max_pooling2d(conv3, pool_size=[2, 2], strides=stride)

        # dense layer
        # reshape the output from the third convolution for the fully connected layer
        pool_to_flat = tf.reshape(pool3, shape=[-1, last_layer_size * last_layer_size * m])
        dense = tf.layers.dense(inputs=pool_to_flat, units=d, activation=tf.nn.relu)
        dropout = tf.layers.dropout(inputs=dense, rate=0.4, training=(op == OP_TRAINING))

        # logits Layer
        y_logits = tf.layers.dense(inputs=dropout, units=label_class_size)
        y = tf.nn.softmax(y_logits)

        cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=y_logits, labels=_y)
        cross_entropy = tf.reduce_mean(cross_entropy) * 100

        # accuracy of the trained model, between 0 (worst) and 1 (best)
        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(_y, 1))

        with tf.name_scope("accuracy"):
            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
            tf.summary.scalar("accuracy", accuracy)

        # training step, the learning rate is a placeholder
        # the learning rate is: # 0.0001 + 0.003 * (1/e)^(step/2000)), i.e. exponential decay from 0.003->0.0001
        lr = 0.0001 + tf.train.exponential_decay(0.003, step, 2000, 1 / math.e)
        train_step = tf.train.AdamOptimizer(lr).minimize(cross_entropy)

        summ = tf.summary.merge_all()

        # init
        init = tf.global_variables_initializer()
        saver = tf.train.Saver()
        sess = tf.Session()
        sess.run(init)

        writer = tf.summary.FileWriter(LOGDIR)
        writer.add_graph(sess.graph)

        if OP_TRAINING == op:
            for i in range(0, iterations):
                a, c, l, s = sess.run([accuracy, cross_entropy, lr, summ],
                                         feed_dict={
                                             x: batch_x,
                                             _y: batch_y,
                                             step: i
                                         })
                print "[ %r ] accuracy: %r, loss: %r, learning-rate: %r" % (i, a, c, l)

                writer.add_summary(s, i)

                # the backpropagation training step
                sess.run(train_step, {x: batch_x, _y: batch_y, step: i})

            # save nn_models parameters after training
            saver.save(sess, nn_model_state_dir)
            return True

        elif OP_TESTING == op:
            saver.restore(sess, nn_model_state_dir)

            a, c, l, s = sess.run([accuracy, cross_entropy, lr, summ],
                     feed_dict={
                         x: batch_x,
                         _y: batch_y,
                         step: 0
                     })

            writer.add_summary(s, 0)

            return a, c, l

        elif OP_PREDICT == op:
            saver.restore(sess, nn_model_state_dir)

            predict, s = sess.run([y, in_summ], feed_dict={x: batch_x})

            writer.add_summary(s, 0)

            return predict

        else:
            print "op not found"

    @staticmethod
    def get_nn_model_ckpt():
        home_path = expanduser("~")
        dir_path = os.path.join(home_path, NN_MODEL_STATE_DIR)

        if os.path.exists(dir_path) is False:
            os.mkdir(dir_path)

        return os.path.join(dir_path, "model.ckpt")