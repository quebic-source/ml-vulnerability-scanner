import sys
import math
import tensorflow as tf
import numpy as np
import os
from os.path import expanduser

NN_MODEL_STATE_DIR = "ai-code-scanner-nn-state-test"

ITERATIONS = 2000
INPUT_SIZE = 50
INPUT_CHANNEL_SIZE = 3
LABEL_CLASS_SIZE = 2

LOGDIR = "/home/tharanga/tensorflow_logs"


# https://github.com/GoogleCloudPlatform/tensorflow-without-a-phd/blob/master/tensorflow-mnist-tutorial/mnist_3.0_convolutional.py
class NeuralNetworkModel:

    def __init__(self):
        pass

    def train(self, batch_x, batch_y):
        self.model(batch_x, batch_y, True)

    def test(self, batch_x, batch_y):
        self.model(batch_x, batch_y, False)

    def model(self, batch_x, batch_y, is_training):

        # eg: 28x28 > 14x14 > 7x7
        # https://www.quora.com/How-can-I-calculate-the-size-of-output-of-convolutional-layer
        last_layer_size = (INPUT_SIZE / 2) / 2 / 2

        # input
        with tf.name_scope("input"):
            x = tf.placeholder(tf.float32, [None, INPUT_SIZE, INPUT_SIZE, INPUT_CHANNEL_SIZE])
            x_visual = tf.reshape(x, [-1, INPUT_SIZE, INPUT_SIZE, INPUT_CHANNEL_SIZE])
            tf.summary.image('input_images', x_visual, 50)

        # labels
        _y = tf.placeholder(tf.float32, [None, LABEL_CLASS_SIZE])

        # step for variable learning rate
        step = tf.placeholder(tf.int32)

        nn_model_ckpt_dir = NeuralNetworkModel.get_nn_model_ckpt_dir()

        # three convolutional layers with their channel counts, and a
        k = 4  # first convolutional layer output depth
        l = 8  # second convolutional layer output depth
        m = 16  # third convolutional layer

        d = 200  # fully connected layer

        # convolutional layer #1
        stride = 2
        conv1 = tf.layers.conv2d(x, filters=k, kernel_size=[10, 10], padding="same", activation=tf.nn.relu)
        pool1 = tf.layers.max_pooling2d(conv1, pool_size=[2, 2], strides=stride)

        # convolutional layer #2
        stride = 2
        conv2 = tf.layers.conv2d(pool1, filters=l, kernel_size=[5, 5], padding="same", activation=tf.nn.relu)
        pool2 = tf.layers.max_pooling2d(conv2, pool_size=[2, 2], strides=stride)

        # convolutional layer #3
        stride = 2
        conv3 = tf.layers.conv2d(pool2, filters=m, kernel_size=[5, 5], padding="same", activation=tf.nn.relu)
        pool3 = tf.layers.max_pooling2d(conv3, pool_size=[2, 2], strides=stride)

        # dense layer
        # reshape the output from the third convolution for the fully connected layer
        pool_to_flat = tf.reshape(pool3, shape=[-1, last_layer_size * last_layer_size * m])
        dense = tf.layers.dense(inputs=pool_to_flat, units=d, activation=tf.nn.relu)
        dropout = tf.layers.dropout(inputs=dense, rate=0.4, training=is_training)

        # logits Layer
        y_logits = tf.layers.dense(inputs=dropout, units=LABEL_CLASS_SIZE)
        y = tf.nn.softmax(y_logits)

        cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=y_logits, labels=_y)
        cross_entropy = tf.reduce_mean(cross_entropy) * 100

        # accuracy of the trained model, between 0 (worst) and 1 (best)
        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(_y, 1))

        with tf.name_scope("accuracy"):
            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
            tf.summary.scalar("accuracy", accuracy)

        # training step, the learning rate is a placeholder
        # the learning rate is: # 0.0001 + 0.003 * (1/e)^(step/2000)), i.e. exponential decay from 0.003->0.0001
        lr = 0.0001 + tf.train.exponential_decay(0.003, step, 2000, 1 / math.e)
        train_step = tf.train.AdamOptimizer(lr).minimize(cross_entropy)

        summ = tf.summary.merge_all()

        # init
        init = tf.global_variables_initializer()
        saver = tf.train.Saver()
        sess = tf.Session()
        sess.run(init)

        writer = tf.summary.FileWriter(LOGDIR)
        writer.add_graph(sess.graph)

        if is_training:

            for i in range(0, ITERATIONS):
                '''
                conv1_r, conv2_r, conv3_r = sess.run([conv1, conv2, conv3],
                                                     feed_dict={
                                                         x: batch_x,
                                                         _y: batch_y
                                                     })
                print "\n"
                print "i", i
                print "conv1_r", np.array(conv1_r).shape
                print "conv2_r", np.array(conv2_r).shape
                print "conv3_r", np.array(conv3_r).shape
                print "\n"
                '''

                a, c, l, s = sess.run([accuracy, cross_entropy, lr, summ],
                                         feed_dict={
                                             x: batch_x,
                                             _y: batch_y,
                                             step: i
                                         })
                print "[ %r ] accuracy: %r, loss: %r, learning-rate: %r" % (i, a, c, l)

                writer.add_summary(s, i)

                # the backpropagation training step
                sess.run(train_step, {x: batch_x, _y: batch_y, step: i})

            saver.save(sess, nn_model_ckpt_dir)
        else:
            saver.restore(sess, nn_model_ckpt_dir)
            predicts_result = sess.run(y, feed_dict={x: batch_x})

            print "[ test ] predicts_shape", np.array(predicts_result).shape

            for predict in predicts_result:
                print "---------"
                print "predict", predict
                print "predict sum", np.sum(predict)

                p0 = float(predict[0])
                p1 = float(predict[1])
                print "class-0", p0
                print "class-1", p1

                if p0 > p1:
                    print "class", "v1"
                else:
                    print "class", "v2"

                print "---------"

    @staticmethod
    def get_nn_model_ckpt_dir():
        home_path = expanduser("~")
        dir_path = os.path.join(home_path, NN_MODEL_STATE_DIR)

        if os.path.exists(dir_path) is False:
            os.mkdir(dir_path)

        return os.path.join(dir_path, 'model.ckpt')


def get_training_data():
    o_lbl = 0
    x_lbl = 1

    o1 = [
        [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],
        [[1, 1, 1], [0, 0, 0], [1, 1, 1], [0, 0, 0], [0, 0, 0]],
        [[0, 0, 0], [1, 1, 1], [0, 0, 0], [1, 1, 1], [0, 0, 0]],
        [[1, 1, 1], [0, 0, 0], [1, 1, 1], [0, 0, 0], [0, 0, 0]],
        [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],
    ]

    o2 = [
        [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],
        [[0, 0, 0], [1, 1, 1], [0, 0, 0], [1, 1, 1], [0, 0, 0], [0, 0, 0]],
        [[0, 0, 0], [0, 0, 0], [1, 1, 1], [0, 0, 0], [1, 1, 1], [0, 0, 0]],
        [[0, 0, 0], [1, 1, 1], [0, 0, 0], [1, 1, 1], [0, 0, 0], [0, 0, 0]],
        [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],
        [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],
    ]

    x1 = [
        [[1, 1, 1], [0, 0, 0], [1, 1, 1]],
        [[0, 0, 0], [1, 1, 1], [0, 0, 0]],
        [[1, 1, 1], [0, 0, 0], [1, 1, 1]],
    ]

    x2 = [
        [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],
        [[0, 0, 0], [1, 1, 1], [0, 0, 0], [1, 1, 1], [0, 0, 0], [0, 0, 0]],
        [[0, 0, 0], [0, 0, 0], [1, 1, 1], [0, 0, 0], [0, 0, 0], [0, 0, 0]],
        [[0, 0, 0], [1, 1, 1], [0, 0, 0], [1, 1, 1], [0, 0, 0], [0, 0, 0]],
        [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],
        [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],
    ]

    x_list = [o1, o2, x1, x2]
    batch_x = []

    for x in x_list:
        x_padded = __padding(x)
        batch_x.append(x_padded)

    batch_y = onehot_encode([o_lbl, o_lbl, x_lbl, x_lbl])

    return batch_x, batch_y


def get_testing_data():
    o_lbl = 0
    x_lbl = 1

    o1 = [
        [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],
        [[0, 0, 0], [1, 1, 1], [0, 0, 0], [1, 1, 1], [0, 0, 0]],
        [[0, 0, 0], [0, 0, 0], [1, 1, 1], [0, 0, 0], [1, 1, 1]],
        [[0, 0, 0], [1, 1, 1], [0, 0, 0], [1, 1, 1], [0, 0, 0]],
        [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],
    ]

    x1 = [
        [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],
        [[0, 0, 0], [1, 1, 1], [0, 0, 0], [1, 1, 1], [0, 0, 0]],
        [[0, 0, 0], [0, 0, 0], [1, 1, 1], [0, 0, 0], [0, 0, 0]],
        [[0, 0, 0], [1, 1, 1], [0, 0, 0], [1, 1, 1], [0, 0, 0]],
        [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],
    ]

    x_list = [o1, x1]
    batch_x = []

    for x in x_list:
        x_padded = __padding(x)
        batch_x.append(x_padded)

    batch_y = onehot_encode([o_lbl, x_lbl])
    return batch_x, batch_y


def __padding(matrix):
    np_array = np.array(matrix)
    x, y, _ = np_array.shape

    if INPUT_SIZE < x:
        raise Exception("input matrix size must be less than < %d x %d" % (INPUT_SIZE, INPUT_SIZE))

    # since x and y always same. we use only x
    pad_total = INPUT_SIZE - x

    if pad_total == 0:
        pad_1 = 0
        pad_2 = 0
    elif (pad_total % 2) == 0:
        pad_1 = pad_total / 2
        pad_2 = pad_1
    else:

        # eg:
        # pad_total = 5
        # pad_1 = pad_total/2 => 5 / 2 = 2
        # pad_2 = pad_1 + 1 => 2 + 1 = 3
        # pad_total == pad_1 + pad_2
        pad_1 = pad_total / 2
        pad_2 = pad_1 + 1

    return np.pad(np_array, ((pad_1, pad_2), (pad_1, pad_2), (0, 0)), 'constant', constant_values=0)


def onehot_encode(labels):
    labels_np = np.array(labels).astype(dtype=np.uint8)
    return (np.arange(LABEL_CLASS_SIZE) == labels_np[:, None]).astype(np.float32)


def des_input_data(x, y):
    print "x.shape", np.array(x).shape
    print "y.shape", np.array(y).shape


if __name__ == "__main__":

    nn = NeuralNetworkModel()

    is_train = int(sys.argv[1])

    if is_train == 1:
        print "training"
        x_batch, y_batch = get_training_data()
        des_input_data(x_batch, y_batch)
        nn.train(x_batch, y_batch)
    else:
        print "testing"
        x_batch, y_batch = get_testing_data()
        des_input_data(x_batch, y_batch)
        nn.test(x_batch, y_batch)


